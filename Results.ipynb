{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from src.SVM_baseline import SVM_experiment\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_SVM_result = {'dataset': [], 'perf_name': [], 'perf_val': []}\n",
    "list_dataset = ['DB_S']\n",
    "list_C = [0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]\n",
    "list_ifold = range(5)\n",
    "\n",
    "\n",
    "for dataset in list_dataset:\n",
    "    y_filename = 'data/' + dataset + '/' + dataset + '_y.npy'\n",
    "    Y = np.load(y_filename)\n",
    "    list_folds = pickle.load(open('data/' + dataset + '/' + dataset + '_folds.data', 'rb'))\n",
    "    temp =Â {}\n",
    "    best_param, best_perf = None, 0.\n",
    "    for C in list_C:\n",
    "        temp[C] = {'auc': [], 'aupr': []}\n",
    "        for ifold in list_ifold:\n",
    "            y_te = Y[list_folds[ifold]]\n",
    "            name = 'result/SVM_exp_' + dataset + '_C:' + str(C) + '_i:' + str(ifold) + '.data'\n",
    "            model = pickle.load(name, 'rb')\n",
    "            list_pred = model.cv_nested_pred\n",
    "            auc, aupr = [], []\n",
    "            for pred in list_pred:\n",
    "                auc.append(metrics.roc_auc_score(y_true=y_te, y_score=pred))\n",
    "                aupr.append(metrics.average_precision_score(y_true=y_te, y_score=pred))\n",
    "            temp[C]['auc'] += auc\n",
    "            temp[C]['aupr'] += aupr\n",
    "        if np.mean(temp[C]['aupr']) > best_perf:\n",
    "            best_param, best_perf = C, np.mean(temp[C]['aupr'])\n",
    "    for i in range(len(temp[best_param]['aupr'])):\n",
    "        dict_SVM_result['dataset'].append(dataset)\n",
    "        dict_SVM_result['perf_name'].append('auc')\n",
    "        dict_SVM_result['perf_val'].append(auc)\n",
    "        dict_SVM_result['dataset'].append(dataset)\n",
    "        dict_SVM_result['perf_name'].append('aupr')\n",
    "        dict_SVM_result['perf_val'].append(aupr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_SVM = pd.DataFrame(dict_SVM_result)\n",
    "df_SVM.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, list_ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "sns.boxplot(x=\"dataset\", y=\"perf_val\", hue=\"perf_name\", data=df_SVM, palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_param_dict(filename):\n",
    "    file_list = filename.split('/')[1].split('_')\n",
    "    print(file_list)\n",
    "    list_cle = ['dataset', 'P_architecture', 'M_architecture',\n",
    "                'batch_size', 'init_lr', 'decay_steps',\n",
    "                'lr_decay_factor', 'early_stopping_counter', 'nb_epochs',\n",
    "                'nb_fully_con_units', 'dropout_keep_prob', 'balance_class', 'stream_type']\n",
    "    param_dict = {cle: file_list[i] for i, cle in enumerate(list_cle)}\n",
    "    if param_dict['M_architecture'] == 'ConvModel':\n",
    "        list_cle += ['M_nb_emb_layers', 'M_hidden_units_emb', 'M_hidden_units_up', 'M_l2_reg_coef']\n",
    "\n",
    "    if param_dict['P_architecture'] == 'ConvModel':\n",
    "        list_cle += ['P_nb_filters', 'P_filter_size', 'P_l2_reg_coef']\n",
    "        if self.P_conv_strides != 1:\n",
    "            list_cle.append('P_conv_strides')\n",
    "    param_dict = {cle: file_list[i] for i, cle in enumerate(list_cle)}\n",
    "\n",
    "    return param_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_reg = ['M_l2_reg_coef', 'P_l2_reg_coef', 'dropout_keep_prob']\n",
    "\n",
    "dict_param_per_archi = {'P_ConvModel': ['P_nb_filters', 'P_filter_size', 'P_conv_strides', 'nb_fully_con_units'],\n",
    "                        'M_ConvModel': ['M_nb_emb_layers', 'M_hidden_units_emb', 'M_hidden_units_up', 'nb_fully_con_units']}\n",
    "\n",
    "param_per_learning_process = ['batch_size', 'init_lr', 'lr_decay_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.param_selection.gridsearch import str_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list_dataset = ['CellLoc', 'SCOPe']\n",
    "#list_architecture = ['ConvModel', 'ConvConcatModel', 'biConvModel']\n",
    "#list_init_lr = [0.001, 0.0001, 0.00001]\n",
    "#list_lr_decay_factor = [0.999, 0.99, 0.9, 0.8]\n",
    "\n",
    "dict_result_lp = {'dataset': [], 'model': [], 'archi_param': [], 'learning_param': [], 'perf_name': [], 'perf_val': []}\n",
    "dict_result_ap_best, dict_result_mp_best = {}, {}\n",
    "\n",
    "\n",
    "\n",
    "for file in glob.glob(\"result/*\"):\n",
    "    param_dict = get_param_dict(file)\n",
    "    if param_dict is not None and param_dict['stream_type'] == 'TFrecords':\n",
    "        auc, aupr = pickle.load(open(file, 'rb'))\n",
    "        auc, aupr = auc[1], aupr[1]  # we take outer test result\n",
    "        print(file)\n",
    "        print(param_dict)\n",
    "        dataset, archi = param_dict['dataset'], '_&&_'.join([param_dict['P_architecture'], param_dict['M_architecture']])\n",
    "        archi_param = '_'.join([str(param_dict[cle]) for cle in dict_param_per_archi[param_dict['P_architecture']]]) + \\\n",
    "            '&& ' + '_'.join([str(param_dict[cle]) for cle in dict_param_per_archi[param_dict['M_architecture']]])\n",
    "        learning_param = '_'.join([str(param_dict[cle]) for cle in param_per_learning_process])\n",
    "        \n",
    "        reg_param = '_'.join([str(param_dict[cle]) for cle in param_reg])\n",
    "        learning_param = '_;_'.join([learning_param, reg_param])\n",
    "        \n",
    "        print('####', archi_param)\n",
    "        for i in range(len(auc)):\n",
    "            dict_result_lp['dataset'].append(dataset)\n",
    "            dict_result_lp['model'].append(archi)\n",
    "            dict_result_lp['archi_param'].append(archi_param)\n",
    "            dict_result_lp['learning_param'].append(learning_param)\n",
    "            dict_result_lp['perf_name'].append('auc')\n",
    "            dict_result_lp['perf_val'].append(np.mean(auc[i]))\n",
    "        for i in range(len(aupr)):\n",
    "            dict_result_lp['dataset'].append(dataset)\n",
    "            dict_result_lp['model'].append(archi)\n",
    "            dict_result_lp['archi_param'].append(archi_param)\n",
    "            dict_result_lp['learning_param'].append(learning_param)\n",
    "            dict_result_lp['perf_name'].append('aupr')\n",
    "            dict_result_lp['perf_val'].append(np.mean(aupr[i]))\n",
    "\n",
    "        if dataset not in dict_result_ap_best.keys():\n",
    "            dict_result_ap_best[dataset] = {}\n",
    "        if archi not in dict_result_ap_best[dataset].keys():\n",
    "            dict_result_ap_best[dataset][archi] = {}\n",
    "        if archi_param not in dict_result_ap_best[dataset][archi].keys():\n",
    "            dict_result_ap_best[dataset][archi][archi_param] = [0., [], []]\n",
    "        if np.mean([np.mean(aupr[i]) for i in range(len(aupr))]) > dict_result_ap_best[dataset][archi][archi_param][0]:\n",
    "            dict_result_ap_best[dataset][archi][archi_param][0] = np.mean([np.mean(aupr[i]) for i in range(len(aupr))])\n",
    "            dict_result_ap_best[dataset][archi][archi_param][1] = auc\n",
    "            dict_result_ap_best[dataset][archi][archi_param][2] = aupr\n",
    "\n",
    "        if dataset not in dict_result_mp_best.keys():\n",
    "            dict_result_mp_best[dataset] = {}\n",
    "        if archi not in dict_result_mp_best[dataset].keys():\n",
    "            dict_result_mp_best[dataset][archi] = [0., [], []]\n",
    "        if np.mean([np.mean(aupr[i]) for i in range(len(aupr))]) > dict_result_mp_best[dataset][archi][0]:\n",
    "            dict_result_mp_best[dataset][archi][0] = np.mean([np.mean(aupr[i]) for i in range(len(aupr))])\n",
    "            dict_result_mp_best[dataset][archi][1] = auc\n",
    "            dict_result_mp_best[dataset][archi][2] = aupr\n",
    "\n",
    "dict_result_ap = {'dataset': [], 'model': [], 'archi_param': [], 'perf_name': [], 'perf_val': []}\n",
    "for dataset in dict_result_ap_best.keys():\n",
    "    for archi in dict_result_ap_best[dataset].keys():\n",
    "        for archi_param in dict_result_ap_best[dataset][archi].keys():\n",
    "            auc, aupr = dict_result_ap_best[dataset][archi][archi_param][1], dict_result_ap_best[dataset][archi][archi_param][2]\n",
    "            for i in range(len(auc)):\n",
    "                dict_result_ap['dataset'].append(dataset)\n",
    "                dict_result_ap['model'].append(archi)\n",
    "                dict_result_ap['archi_param'].append(archi_param)\n",
    "                dict_result_ap['perf_name'].append('auc')\n",
    "                dict_result_ap['perf_val'].append(np.mean(auc[i]))\n",
    "            for i in range(len(aupr)):\n",
    "                dict_result_ap['dataset'].append(dataset)\n",
    "                dict_result_ap['model'].append(archi)\n",
    "                dict_result_ap['archi_param'].append(archi_param)\n",
    "                dict_result_ap['perf_name'].append('aupr')\n",
    "                dict_result_ap['perf_val'].append(np.mean(aupr[i]))\n",
    "\n",
    "\n",
    "dict_result_mp = {'dataset': [], 'model': [], 'perf_name': [], 'perf_val': []}\n",
    "for dataset in dict_result_mp_best.keys():\n",
    "    for archi in dict_result_mp_best[dataset].keys():\n",
    "        auc, aupr = dict_result_mp_best[dataset][archi][1], dict_result_mp_best[dataset][archi][2]\n",
    "        for i in range(len(auc)):\n",
    "            dict_result_mp['dataset'].append(dataset)\n",
    "            dict_result_mp['model'].append(archi)\n",
    "            dict_result_mp['perf_name'].append('auc')\n",
    "            dict_result_mp['perf_val'].append(np.mean(auc[i]))\n",
    "        for i in range(len(aupr)):\n",
    "            dict_result_mp['dataset'].append(dataset)\n",
    "            dict_result_mp['model'].append(archi)\n",
    "            dict_result_mp['perf_name'].append('aupr')\n",
    "            dict_result_mp['perf_val'].append(np.mean(aupr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perf for the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archi_result = pd.DataFrame(dict_result_mp)\n",
    "archi_result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nb_plot = len(dict_result_mp_best.keys())\n",
    "print(nb_plot)\n",
    "f, list_ax = plt.subplots(nb_plot, 1, figsize=(7, 7 * nb_plot))\n",
    "if nb_plot == 1:\n",
    "    list_ax = [list_ax]\n",
    "else:\n",
    "    print('ok')\n",
    "\n",
    "i = 0\n",
    "for dataset in dict_result_mp_best.keys():\n",
    "    print(dataset)\n",
    "    local_result = archi_result[archi_result['dataset']==dataset]\n",
    "    title = dataset\n",
    "    sns.boxplot(x=\"model\", y=\"perf_val\", hue=\"perf_name\", data=local_result, palette=\"Set3\", ax=list_ax[i]).set_title(title) \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perf for the best architecture parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archiparam_result = pd.DataFrame(dict_result_ap)\n",
    "archiparam_result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nb_plot = sum([len(dict_result_ap_best[dataset].keys()) for dataset in dict_result_ap_best.keys()])\n",
    "f, list_ax = plt.subplots(nb_plot, 1, figsize=(20, 7 * nb_plot))\n",
    "i=0\n",
    "for dataset in dict_result_ap_best.keys():\n",
    "    print(dataset)\n",
    "    local_result_1 = archiparam_result[archiparam_result['dataset']==dataset]\n",
    "    for archi in dict_result_ap_best[dataset].keys():\n",
    "        print(archi)\n",
    "        local_result = local_result_1[local_result_1['model']==archi]\n",
    "        title = dataset + '_' + archi\n",
    "        if nb_plot > 1:\n",
    "            sns.boxplot(x=\"archi_param\", y=\"perf_val\", hue=\"perf_name\", data=local_result, palette=\"Set3\", ax=list_ax[i]).set_title(title)\n",
    "            list_ax[i].set_xticklabels(list_ax[i].get_xticklabels(), rotation=80)\n",
    "        else:\n",
    "            sns.boxplot(x=\"archi_param\", y=\"perf_val\", hue=\"perf_name\", data=local_result, palette=\"Set3\", ax=list_ax).set_title(title)\n",
    "            list_ax.set_xticklabels(list_ax.get_xticklabels(),rotation=80)\n",
    "        i+=1\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perf of a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learningparam_result = pd.DataFrame(dict_result_lp)\n",
    "learningparam_result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset, archi, archi_param = 'DB_S', 'ConvModel_&&_ConvModel', '(40,80)_6_(100)_1_&&_3_(50)_(50)'\n",
    "\n",
    "local_result = learningparam_result[learningparam_result['dataset']==dataset]\n",
    "local_result = local_result[local_result['model']==archi]\n",
    "local_result = local_result[local_result['archi_param']==archi_param]\n",
    "title = dataset + '_' + archi + '_' + archi_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, list_ax = plt.subplots(1, 1, figsize=(18, 7))\n",
    "\n",
    "sns.boxplot(x=\"learning_param\", y=\"perf_val\", hue=\"perf_name\", data=local_result, palette=\"Set3\", ax=list_ax).set_title(title)\n",
    "list_ax.set_xticklabels(list_ax.get_xticklabels(),rotation=30)\n",
    "i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
